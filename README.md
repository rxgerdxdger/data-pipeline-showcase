# ğŸ“Š Superstore Sales ETL Pipeline

A complete **Extract, Transform, Load (ETL)** data pipeline project that processes Kaggle's Superstore sales dataset, cleans the data, and loads it into an SQLite database for analysis.

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-orange)](https://jupyter.org/)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)

---

## ğŸ“‹ Project Overview

This ETL pipeline demonstrates end-to-end data engineering skills including:
- **Extracting** data from Kaggle using KaggleHub API
- **Transforming** messy data by handling duplicates, missing values, and data type issues
- **Loading** cleaned data into a SQLite database
- **Validating** data quality improvements with SQL queries

**Key Achievement**: Improved data quality from 12,617 raw records to 11,737 clean records (93% retention rate) by removing duplicates and handling missing critical fields.

---

## ğŸ¯ Skills Demonstrated

- **Python Programming**: pandas, logging, datetime
- **Data Cleaning**: Duplicate removal, missing value handling, data type conversion
- **ETL Design**: Modular pipeline with extract/transform/load phases
- **Database Operations**: SQLite integration, table creation, bulk loading
- **Data Quality Management**: Validation, error handling, logging
- **Documentation**: Clear code comments and project documentation

---

## ğŸ“Š Data Quality Improvements

| Metric | Before ETL | After ETL | Improvement |
|--------|-----------|-----------|-------------|
| **Total Records** | 12,617 | 11,737 | -880 bad records |
| **Duplicates** | Present | 0 | 100% removed |
| **Missing Dates** | Present | 0 | 100% handled |
| **Missing Critical Fields** | Present | 0 | 100% cleaned |
| **Data Quality** | ğŸŸ¡ Poor | âœ… High | +93% |

---

## ğŸš€ Quick Start

### Prerequisites

```bash
pip install pandas kagglehub jupyter
```

### Running the Pipeline

1. **Clone or download this project**
2. **Open the Jupyter Notebook**
```bash
jupyter notebook salesetl_corrected-1.ipynb
```
3. **Run all cells** to execute the complete ETL pipeline
4. **Query the database** using the provided SQL examples

---

## ğŸ“ Project Structure

```
superstore-etl-pipeline/
â”‚
â”œâ”€â”€ salesetl_corrected-1.ipynb    # Main ETL pipeline notebook
â”œâ”€â”€ README.md                      # Project documentation (this file)
â”œâ”€â”€ QUICKSTART.md                  # 5-minute setup guide
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ LICENSE                        # MIT license
â”œâ”€â”€ .gitignore                     # Git ignore patterns
â”‚
â””â”€â”€ docs/
    â””â”€â”€ ETL_PROCESS.md             # Detailed ETL documentation
```

---

## ğŸ”„ ETL Pipeline Workflow

### 1. Extract ğŸ“¥
- Download Superstore sales dataset from Kaggle
- Load CSV file into pandas DataFrame
- Initial: **12,617 rows Ã— 19 columns**

### 2. Transform ğŸ”§
- Remove duplicate records
- Handle missing dates
- Convert date strings to datetime
- Fill missing categories with 'Unknown'
- Remove incomplete records
- Calculate `total_amount = Quantity Ã— Sales`
- Add processing timestamp
- Final: **11,737 rows Ã— 21 columns**

### 3. Load ğŸ’¾
- Create SQLite database: `superstore_sales_clean.db`
- Bulk insert cleaned records
- Verify with SQL queries

---

## ğŸ“ˆ Technologies Used

| Technology | Purpose |
|-----------|---------|
| **Python 3.8+** | Core programming language |
| **Pandas** | Data manipulation |
| **KaggleHub** | Dataset download |
| **SQLite3** | Database storage |
| **Jupyter** | Interactive development |
| **Logging** | Pipeline monitoring |

---

## ğŸ”— Connect to BI Tools

### Power BI
1. Get Data â†’ SQLite
2. Browse to `superstore_sales_clean.db`
3. Select `SUPERSTORE_SALES_CLEAN` table

### Tableau
1. Connect to Data â†’ SQLite
2. Select database file
3. Start building dashboards

---

## ğŸ“ Next Steps

**Beginner:**
- Add data validation rules
- Export to CSV format
- Create summary reports

**Intermediate:**
- Implement incremental loading
- Add email notifications
- Build data quality dashboard

**Advanced:**
- Deploy on AWS/Azure
- Integrate Apache Airflow
- Build data warehouse

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file

---

## ğŸ‘¤ Author

**Your Name**
- GitHub: [@yourusername](https://github.com/yourusername)
- LinkedIn: [linkedin.com/in/yourprofile](https://linkedin.com/in/yourprofile)
- Email: your.email@example.com

---

## ğŸ™ Acknowledgments

- Dataset: [Kaggle Superstore Sales](https://www.kaggle.com/datasets/dataobsession/superstore-sales-the-data-quality-challenge)
- Built for data engineering portfolio development

---

â­ **If you found this helpful, please star the repo!**

**Built with â¤ï¸ for Data Engineering**
